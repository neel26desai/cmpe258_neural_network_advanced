{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdc2NcKEvSmkMJW0prFMIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neel26desai/cmpe258_neural_network_advanced/blob/main/Neural_Networks_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL2m6NjXQY8O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Defining the data\n",
        "n, d = 1000, 3  # 1000 records, 3 features\n",
        "\n",
        "# Creating x, values will be between -1 and 1 and the shape will be n, d\n",
        "x = np.random.uniform(-1, 1, (n, d))\n",
        "\n",
        "# Creating weights that allow us to get the y_true values\n",
        "weights_true = np.array([[1], [3], [1]], dtype=np.float32)\n",
        "bias_true = np.array([1], dtype=np.float32)\n",
        "\n",
        "# Creating an equation y_true using np.dot for matrix multiplication\n",
        "y_true = np.dot(x**3, weights_true) + \\\n",
        "         np.dot(x**2, weights_true) + \\\n",
        "         np.dot(x, weights_true) + bias_true\n",
        "\n",
        "print(f'x: {x.shape}, weights: {weights_true.shape}, bias: {bias_true.shape}, y: {y_true.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUNoGDQoQj6f",
        "outputId": "b725ba3e-5efb-45e1-eeb9-e6e890b1bb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: (1000, 3), weights: (3, 1), bias: (1,), y: (1000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y_true, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "x9PWwQwyQlQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Model"
      ],
      "metadata": {
        "id": "1fwUZr9iFfr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1SpRXaxJFfjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(3, 128),  # First hidden layer\n",
        "  nn.ReLU(),  # Activation function\n",
        "  nn.Linear(128, 64),  # Second hidden layer\n",
        "  nn.ReLU(),  # Activation function\n",
        "  nn.Linear(64, 1)  # Output layer\n",
        ")\n",
        "\n",
        "# Print the model\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "Ql_8sJLjQnbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce148b7-d108-460c-ba64-e938053b48b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "  model.train()  # Set the model to training mode\n",
        "  y_pred = model(torch.tensor(X_train, dtype=torch.float32))\n",
        "  loss = loss_fn(y_pred, torch.tensor(y_train, dtype=torch.float32))\n",
        "\n",
        "  optimizer.zero_grad()  # Clear the gradients\n",
        "  loss.backward()  # Compute the gradients\n",
        "  optimizer.step()  # Update the parameters\n",
        "\n",
        "  # Validate the model\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  with torch.inference_mode():\n",
        "    y_pred_test = model(torch.tensor(X_test, dtype=torch.float32))\n",
        "    loss_test = loss_fn(y_pred_test, torch.tensor(y_test, dtype=torch.float32))\n",
        "\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss:.3f}, Test Loss: {loss_test:.3f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "1DW6-51w3dip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b94ae9e-d71e-4822-c341-a794e929d338"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 0.328, Test Loss: 0.414\n",
            "Epoch: 1, Train Loss: 0.417, Test Loss: 0.277\n",
            "Epoch: 2, Train Loss: 0.300, Test Loss: 0.401\n",
            "Epoch: 3, Train Loss: 0.398, Test Loss: 0.312\n",
            "Epoch: 4, Train Loss: 0.338, Test Loss: 0.428\n",
            "Epoch: 5, Train Loss: 0.430, Test Loss: 0.279\n",
            "Epoch: 6, Train Loss: 0.302, Test Loss: 0.379\n",
            "Epoch: 7, Train Loss: 0.374, Test Loss: 0.317\n",
            "Epoch: 8, Train Loss: 0.347, Test Loss: 0.434\n",
            "Epoch: 9, Train Loss: 0.432, Test Loss: 0.297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#L1 Regularization"
      ],
      "metadata": {
        "id": "rb9xWWOeHy7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),  # First hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(128, 64),  # Second hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(64, 1)  # Output layer\n",
        ")\n",
        "\n",
        "# Parameters\n",
        "l1_lambda = 0.01\n",
        "loss_fn = nn.L1Loss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Define the optimizer\n",
        "\n",
        "# Convert training and testing data to tensors once, assuming X_train, y_train, X_test, y_test are defined\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "    # Calculate L1 regularization dynamically\n",
        "    l1_reg = l1_lambda * sum(p.abs().sum() for p in model.parameters())\n",
        "\n",
        "    # Forward pass and loss computation\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor) + l1_reg\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Compute the gradients\n",
        "    optimizer.step()  # Update the parameters\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Inference mode, no gradient tracking\n",
        "        y_pred_test = model(X_test_tensor)\n",
        "        test_loss = loss_fn(y_pred_test, y_test_tensor) + l1_reg\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss.item():.3f}, Test Loss: {test_loss.item():.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptpkPwmTFQxv",
        "outputId": "254e6fc9-dec5-473b-a173-064d881d1e69"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 8.421, Test Loss: 8.107\n",
            "Epoch: 1, Train Loss: 8.400, Test Loss: 8.088\n",
            "Epoch: 2, Train Loss: 8.380, Test Loss: 8.070\n",
            "Epoch: 3, Train Loss: 8.360, Test Loss: 8.052\n",
            "Epoch: 4, Train Loss: 8.341, Test Loss: 8.034\n",
            "Epoch: 5, Train Loss: 8.321, Test Loss: 8.016\n",
            "Epoch: 6, Train Loss: 8.301, Test Loss: 7.999\n",
            "Epoch: 7, Train Loss: 8.282, Test Loss: 7.981\n",
            "Epoch: 8, Train Loss: 8.263, Test Loss: 7.964\n",
            "Epoch: 9, Train Loss: 8.244, Test Loss: 7.947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L2"
      ],
      "metadata": {
        "id": "YXrzI6H-HlMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),  # First hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(128, 64),  # Second hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(64, 1)  # Output layer\n",
        ")\n",
        "\n",
        "# Parameters\n",
        "l2_lambda = 0.01\n",
        "loss_fn = nn.MSELoss()  # Use MSE loss for L2 regularization\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Define the optimizer\n",
        "\n",
        "# Convert training and testing data to tensors once, assuming X_train, y_train, X_test, y_test are defined\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "    # Calculate L2 regularization dynamically\n",
        "    l2_reg = l2_lambda / 2 * sum(p.pow(2).sum() for p in model.parameters())\n",
        "\n",
        "    # Forward pass and loss computation\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor) + l2_reg\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Compute the gradients\n",
        "    optimizer.step()  # Update the parameters\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Inference mode, no gradient tracking\n",
        "        y_pred_test = model(X_test_tensor)\n",
        "        test_loss = loss_fn(y_pred_test, y_test_tensor) + l2_reg\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss.item():.3f}, Test Loss: {test_loss.item():.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FbDiyr5GzMO",
        "outputId": "f3f90c34-8feb-4ecc-a317-cba13867cb10"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 19.683, Test Loss: 15.497\n",
            "Epoch: 1, Train Loss: 18.601, Test Loss: 14.634\n",
            "Epoch: 2, Train Loss: 17.582, Test Loss: 13.792\n",
            "Epoch: 3, Train Loss: 16.576, Test Loss: 12.935\n",
            "Epoch: 4, Train Loss: 15.543, Test Loss: 12.044\n",
            "Epoch: 5, Train Loss: 14.459, Test Loss: 11.130\n",
            "Epoch: 6, Train Loss: 13.332, Test Loss: 10.221\n",
            "Epoch: 7, Train Loss: 12.196, Test Loss: 9.354\n",
            "Epoch: 8, Train Loss: 11.094, Test Loss: 8.555\n",
            "Epoch: 9, Train Loss: 10.066, Test Loss: 7.834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),  # First hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Dropout(p=0.2),  # Dropout layer with probability 0.2\n",
        "    nn.Linear(128, 64),  # Second hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Dropout(p=0.2),  # Dropout layer with probability 0.2\n",
        "    nn.Linear(64, 1)  # Output layer\n",
        ")\n",
        "\n",
        "# Parameters\n",
        "l2_lambda = 0.01\n",
        "loss_fn = nn.MSELoss()  # Use MSE loss for L2 regularization\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Define the optimizer\n",
        "\n",
        "# Convert training and testing data to tensors once, assuming X_train, y_train, X_test, y_test are defined\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "    # Calculate L2 regularization dynamically\n",
        "    l2_reg = l2_lambda / 2 * sum(p.pow(2).sum() for p in model.parameters())\n",
        "\n",
        "    # Forward pass and loss computation\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor) + l2_reg\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Compute the gradients\n",
        "    optimizer.step()  # Update the parameters\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Inference mode, no gradient tracking\n",
        "        y_pred_test = model(X_test_tensor)\n",
        "        test_loss = loss_fn(y_pred_test, y_test_tensor) + l2_reg\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss.item():.3f}, Test Loss: {test_loss.item():.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Wpl3J0Htqv",
        "outputId": "d286957c-08cb-4c1a-8f77-de14bf9c0eb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 18.384, Test Loss: 14.341\n",
            "Epoch: 1, Train Loss: 17.293, Test Loss: 13.447\n",
            "Epoch: 2, Train Loss: 16.182, Test Loss: 12.542\n",
            "Epoch: 3, Train Loss: 15.152, Test Loss: 11.622\n",
            "Epoch: 4, Train Loss: 13.894, Test Loss: 10.659\n",
            "Epoch: 5, Train Loss: 12.827, Test Loss: 9.730\n",
            "Epoch: 6, Train Loss: 11.796, Test Loss: 8.852\n",
            "Epoch: 7, Train Loss: 10.813, Test Loss: 8.048\n",
            "Epoch: 8, Train Loss: 9.596, Test Loss: 7.309\n",
            "Epoch: 9, Train Loss: 8.714, Test Loss: 6.648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early Stop"
      ],
      "metadata": {
        "id": "XEbNWQfHIMm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the model with dropout layers\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(64, 1)\n",
        ")\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 2\n",
        "best_loss = float('inf')\n",
        "trigger_times = 0\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(1500):  # Example with a higher number of maximum epochs\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_test = model(X_test_tensor)\n",
        "        test_loss = loss_fn(y_pred_test, y_test_tensor)\n",
        "\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss.item():.3f}, Test Loss: {test_loss.item():.3f}')\n",
        "\n",
        "    # Check if the current loss is lower than the best one seen so far\n",
        "    if test_loss < best_loss or test_loss<1:\n",
        "        best_loss = test_loss\n",
        "        trigger_times = 0  # reset the patience trigger\n",
        "        # Save the best model, if needed\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch} with test loss {test_loss:.3f}\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQliDb9nIDp_",
        "outputId": "2ef9e273-6b1b-4220-ab10-c17872f1d3fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 19.713, Test Loss: 15.080\n",
            "Epoch: 1, Train Loss: 18.364, Test Loss: 14.042\n",
            "Epoch: 2, Train Loss: 17.024, Test Loss: 13.046\n",
            "Epoch: 3, Train Loss: 15.798, Test Loss: 12.070\n",
            "Epoch: 4, Train Loss: 14.707, Test Loss: 11.131\n",
            "Epoch: 5, Train Loss: 13.454, Test Loss: 10.215\n",
            "Epoch: 6, Train Loss: 12.314, Test Loss: 9.354\n",
            "Epoch: 7, Train Loss: 11.497, Test Loss: 8.589\n",
            "Epoch: 8, Train Loss: 10.574, Test Loss: 7.911\n",
            "Epoch: 9, Train Loss: 9.689, Test Loss: 7.277\n",
            "Epoch: 10, Train Loss: 9.318, Test Loss: 6.695\n",
            "Epoch: 11, Train Loss: 8.510, Test Loss: 6.099\n",
            "Epoch: 12, Train Loss: 7.550, Test Loss: 5.548\n",
            "Epoch: 13, Train Loss: 7.568, Test Loss: 5.023\n",
            "Epoch: 14, Train Loss: 6.694, Test Loss: 4.530\n",
            "Epoch: 15, Train Loss: 6.338, Test Loss: 4.095\n",
            "Epoch: 16, Train Loss: 5.745, Test Loss: 3.701\n",
            "Epoch: 17, Train Loss: 5.678, Test Loss: 3.369\n",
            "Epoch: 18, Train Loss: 5.485, Test Loss: 3.053\n",
            "Epoch: 19, Train Loss: 5.145, Test Loss: 2.805\n",
            "Epoch: 20, Train Loss: 4.957, Test Loss: 2.613\n",
            "Epoch: 21, Train Loss: 4.647, Test Loss: 2.436\n",
            "Epoch: 22, Train Loss: 4.701, Test Loss: 2.283\n",
            "Epoch: 23, Train Loss: 4.655, Test Loss: 2.143\n",
            "Epoch: 24, Train Loss: 4.464, Test Loss: 1.971\n",
            "Epoch: 25, Train Loss: 4.001, Test Loss: 1.901\n",
            "Epoch: 26, Train Loss: 3.917, Test Loss: 1.809\n",
            "Epoch: 27, Train Loss: 3.502, Test Loss: 1.722\n",
            "Epoch: 28, Train Loss: 3.973, Test Loss: 1.702\n",
            "Epoch: 29, Train Loss: 3.552, Test Loss: 1.656\n",
            "Epoch: 30, Train Loss: 3.480, Test Loss: 1.614\n",
            "Epoch: 31, Train Loss: 3.721, Test Loss: 1.567\n",
            "Epoch: 32, Train Loss: 3.425, Test Loss: 1.538\n",
            "Epoch: 33, Train Loss: 3.543, Test Loss: 1.465\n",
            "Epoch: 34, Train Loss: 3.342, Test Loss: 1.401\n",
            "Epoch: 35, Train Loss: 3.285, Test Loss: 1.328\n",
            "Epoch: 36, Train Loss: 3.149, Test Loss: 1.337\n",
            "Epoch: 37, Train Loss: 3.168, Test Loss: 1.340\n",
            "Early stopping triggered at epoch 37 with test loss 1.340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Monte Carlo Dropout"
      ],
      "metadata": {
        "id": "YhLvO1SmJF3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a custom dropout module that stays active during evaluation\n",
        "class MCDropout(nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(MCDropout, self).__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training or not self.training:\n",
        "            return F.dropout(x, p=self.p, training=True)\n",
        "        return x\n",
        "\n",
        "# Define the model using the custom MCDropout\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),\n",
        "    nn.ReLU(),\n",
        "    MCDropout(0.5),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    MCDropout(0.5),\n",
        "    nn.Linear(64, 1)\n",
        ")\n",
        "\n",
        "# Example of how to perform MC Dropout during inference\n",
        "def mc_dropout_inference(model, input_tensor, iterations=100):\n",
        "    model.train()  # Keep the dropout layers active\n",
        "    predictions = [model(input_tensor) for _ in range(iterations)]\n",
        "    predictions = torch.stack(predictions)\n",
        "    mean_predictions = predictions.mean(0)\n",
        "    std_predictions = predictions.std(0)\n",
        "    return mean_predictions, std_predictions\n",
        "\n",
        "# Example usage\n",
        "model.eval()  # Not strictly necessary here since dropout is always active\n",
        "mean_preds, std_preds = mc_dropout_inference(model, X_test_tensor)\n",
        "\n",
        "print(\"Mean predictions:\", mean_preds)\n",
        "print(\"Standard deviation of predictions:\", std_preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R89tQGs4IUrr",
        "outputId": "21c7a768-c32e-4ed9-cd44-80242d8f571c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean predictions: tensor([[-0.1195],\n",
            "        [-0.1349],\n",
            "        [-0.0844],\n",
            "        [-0.1241],\n",
            "        [-0.1131],\n",
            "        [-0.0887],\n",
            "        [-0.1240],\n",
            "        [-0.0955],\n",
            "        [-0.0992],\n",
            "        [-0.1021],\n",
            "        [-0.1148],\n",
            "        [-0.1645],\n",
            "        [-0.0394],\n",
            "        [-0.1072],\n",
            "        [-0.1326],\n",
            "        [-0.1387],\n",
            "        [-0.1096],\n",
            "        [-0.0549],\n",
            "        [-0.0638],\n",
            "        [-0.1221],\n",
            "        [-0.1724],\n",
            "        [-0.2202],\n",
            "        [-0.1111],\n",
            "        [-0.1143],\n",
            "        [-0.1509],\n",
            "        [-0.1692],\n",
            "        [-0.1865],\n",
            "        [-0.1004],\n",
            "        [-0.0733],\n",
            "        [-0.2221],\n",
            "        [-0.0914],\n",
            "        [-0.1570],\n",
            "        [-0.1877],\n",
            "        [-0.1072],\n",
            "        [-0.0946],\n",
            "        [-0.1353],\n",
            "        [-0.1337],\n",
            "        [-0.1299],\n",
            "        [-0.1479],\n",
            "        [-0.0997],\n",
            "        [-0.1185],\n",
            "        [-0.0699],\n",
            "        [-0.1251],\n",
            "        [-0.0856],\n",
            "        [-0.0730],\n",
            "        [-0.0547],\n",
            "        [-0.1337],\n",
            "        [-0.1142],\n",
            "        [-0.0701],\n",
            "        [-0.1393],\n",
            "        [-0.1002],\n",
            "        [-0.1831],\n",
            "        [-0.1020],\n",
            "        [-0.1352],\n",
            "        [-0.1129],\n",
            "        [-0.1305],\n",
            "        [-0.1838],\n",
            "        [-0.0751],\n",
            "        [-0.0429],\n",
            "        [-0.1805],\n",
            "        [-0.0922],\n",
            "        [-0.0871],\n",
            "        [-0.0652],\n",
            "        [-0.0712],\n",
            "        [-0.0486],\n",
            "        [-0.1325],\n",
            "        [-0.1216],\n",
            "        [-0.0747],\n",
            "        [-0.1508],\n",
            "        [-0.1117],\n",
            "        [-0.0987],\n",
            "        [-0.1005],\n",
            "        [-0.1764],\n",
            "        [-0.0635],\n",
            "        [-0.0988],\n",
            "        [-0.1870],\n",
            "        [-0.1323],\n",
            "        [-0.1517],\n",
            "        [-0.0962],\n",
            "        [-0.1775],\n",
            "        [-0.0554],\n",
            "        [-0.1600],\n",
            "        [-0.1326],\n",
            "        [-0.1083],\n",
            "        [-0.0921],\n",
            "        [-0.0963],\n",
            "        [-0.1243],\n",
            "        [-0.1021],\n",
            "        [-0.1325],\n",
            "        [-0.0615],\n",
            "        [-0.1634],\n",
            "        [-0.1376],\n",
            "        [-0.0959],\n",
            "        [-0.0609],\n",
            "        [-0.0851],\n",
            "        [-0.1173],\n",
            "        [-0.0798],\n",
            "        [-0.1618],\n",
            "        [-0.0861],\n",
            "        [-0.1350],\n",
            "        [-0.1256],\n",
            "        [-0.1442],\n",
            "        [-0.0839],\n",
            "        [-0.1003],\n",
            "        [-0.1395],\n",
            "        [-0.1424],\n",
            "        [-0.1977],\n",
            "        [-0.0927],\n",
            "        [-0.1193],\n",
            "        [-0.0983],\n",
            "        [-0.1450],\n",
            "        [-0.1330],\n",
            "        [-0.1590],\n",
            "        [-0.0654],\n",
            "        [-0.0782],\n",
            "        [-0.0743],\n",
            "        [-0.1525],\n",
            "        [-0.1136],\n",
            "        [-0.0561],\n",
            "        [-0.0989],\n",
            "        [-0.0421],\n",
            "        [-0.1028],\n",
            "        [-0.1331],\n",
            "        [-0.1168],\n",
            "        [-0.1087],\n",
            "        [-0.0835],\n",
            "        [-0.1066],\n",
            "        [-0.1073],\n",
            "        [-0.1015],\n",
            "        [-0.0574],\n",
            "        [-0.0345],\n",
            "        [-0.0784],\n",
            "        [-0.1367],\n",
            "        [-0.1058],\n",
            "        [-0.1102],\n",
            "        [-0.1167],\n",
            "        [-0.1161],\n",
            "        [-0.1601],\n",
            "        [-0.0936],\n",
            "        [-0.1421],\n",
            "        [-0.1240],\n",
            "        [-0.1010],\n",
            "        [-0.1502],\n",
            "        [-0.1203],\n",
            "        [-0.0854],\n",
            "        [-0.0799],\n",
            "        [-0.2085],\n",
            "        [-0.1108],\n",
            "        [-0.0573],\n",
            "        [-0.0818],\n",
            "        [-0.1199],\n",
            "        [-0.0905],\n",
            "        [-0.0688],\n",
            "        [-0.0470],\n",
            "        [-0.1138],\n",
            "        [-0.1265],\n",
            "        [-0.0984],\n",
            "        [-0.1062],\n",
            "        [-0.1163],\n",
            "        [-0.0842],\n",
            "        [-0.1824],\n",
            "        [-0.1094],\n",
            "        [-0.0722],\n",
            "        [-0.0685],\n",
            "        [-0.1203],\n",
            "        [-0.1192],\n",
            "        [-0.0721],\n",
            "        [-0.0402],\n",
            "        [-0.0821],\n",
            "        [-0.1834],\n",
            "        [-0.1475],\n",
            "        [-0.0938],\n",
            "        [-0.0926],\n",
            "        [-0.1554],\n",
            "        [-0.1036],\n",
            "        [-0.1459],\n",
            "        [-0.1036],\n",
            "        [-0.1669],\n",
            "        [-0.1247],\n",
            "        [-0.1497],\n",
            "        [-0.0519],\n",
            "        [-0.0883],\n",
            "        [-0.0511],\n",
            "        [-0.1013],\n",
            "        [-0.1001],\n",
            "        [-0.1597],\n",
            "        [-0.0679],\n",
            "        [-0.0497],\n",
            "        [-0.0728],\n",
            "        [-0.0975],\n",
            "        [-0.1108],\n",
            "        [-0.1260],\n",
            "        [-0.0241],\n",
            "        [-0.0337],\n",
            "        [-0.0373],\n",
            "        [-0.1251],\n",
            "        [-0.1495],\n",
            "        [-0.1082],\n",
            "        [-0.0683],\n",
            "        [-0.1189]], grad_fn=<MeanBackward1>)\n",
            "Standard deviation of predictions: tensor([[0.1046],\n",
            "        [0.1408],\n",
            "        [0.0996],\n",
            "        [0.1094],\n",
            "        [0.1057],\n",
            "        [0.0967],\n",
            "        [0.0945],\n",
            "        [0.0901],\n",
            "        [0.1123],\n",
            "        [0.1110],\n",
            "        [0.1200],\n",
            "        [0.1218],\n",
            "        [0.0904],\n",
            "        [0.0982],\n",
            "        [0.1176],\n",
            "        [0.1223],\n",
            "        [0.1076],\n",
            "        [0.0902],\n",
            "        [0.0907],\n",
            "        [0.1171],\n",
            "        [0.1320],\n",
            "        [0.1649],\n",
            "        [0.0996],\n",
            "        [0.1070],\n",
            "        [0.1028],\n",
            "        [0.1451],\n",
            "        [0.1446],\n",
            "        [0.1084],\n",
            "        [0.1090],\n",
            "        [0.1684],\n",
            "        [0.0922],\n",
            "        [0.1234],\n",
            "        [0.1470],\n",
            "        [0.1022],\n",
            "        [0.1089],\n",
            "        [0.1219],\n",
            "        [0.1063],\n",
            "        [0.1392],\n",
            "        [0.1389],\n",
            "        [0.1137],\n",
            "        [0.1105],\n",
            "        [0.0963],\n",
            "        [0.1158],\n",
            "        [0.1113],\n",
            "        [0.1392],\n",
            "        [0.0964],\n",
            "        [0.1027],\n",
            "        [0.1025],\n",
            "        [0.1060],\n",
            "        [0.1010],\n",
            "        [0.1213],\n",
            "        [0.1322],\n",
            "        [0.1198],\n",
            "        [0.1215],\n",
            "        [0.1307],\n",
            "        [0.1231],\n",
            "        [0.1589],\n",
            "        [0.1018],\n",
            "        [0.0953],\n",
            "        [0.1348],\n",
            "        [0.1143],\n",
            "        [0.0797],\n",
            "        [0.1119],\n",
            "        [0.1074],\n",
            "        [0.1168],\n",
            "        [0.1191],\n",
            "        [0.0925],\n",
            "        [0.1050],\n",
            "        [0.1025],\n",
            "        [0.1044],\n",
            "        [0.1000],\n",
            "        [0.1056],\n",
            "        [0.1344],\n",
            "        [0.1027],\n",
            "        [0.1060],\n",
            "        [0.1365],\n",
            "        [0.1038],\n",
            "        [0.1181],\n",
            "        [0.0938],\n",
            "        [0.1244],\n",
            "        [0.0852],\n",
            "        [0.1239],\n",
            "        [0.1111],\n",
            "        [0.1200],\n",
            "        [0.0991],\n",
            "        [0.1213],\n",
            "        [0.1181],\n",
            "        [0.0928],\n",
            "        [0.1056],\n",
            "        [0.1346],\n",
            "        [0.1270],\n",
            "        [0.1267],\n",
            "        [0.1035],\n",
            "        [0.0862],\n",
            "        [0.0912],\n",
            "        [0.0964],\n",
            "        [0.1130],\n",
            "        [0.1298],\n",
            "        [0.1140],\n",
            "        [0.1189],\n",
            "        [0.0883],\n",
            "        [0.1230],\n",
            "        [0.1018],\n",
            "        [0.1114],\n",
            "        [0.1148],\n",
            "        [0.1055],\n",
            "        [0.1541],\n",
            "        [0.1050],\n",
            "        [0.1444],\n",
            "        [0.0934],\n",
            "        [0.1356],\n",
            "        [0.1217],\n",
            "        [0.1383],\n",
            "        [0.0986],\n",
            "        [0.1094],\n",
            "        [0.0987],\n",
            "        [0.1344],\n",
            "        [0.1308],\n",
            "        [0.1030],\n",
            "        [0.1159],\n",
            "        [0.0957],\n",
            "        [0.1206],\n",
            "        [0.1177],\n",
            "        [0.0979],\n",
            "        [0.0730],\n",
            "        [0.1150],\n",
            "        [0.1075],\n",
            "        [0.1191],\n",
            "        [0.1129],\n",
            "        [0.0938],\n",
            "        [0.1288],\n",
            "        [0.0944],\n",
            "        [0.1251],\n",
            "        [0.1162],\n",
            "        [0.1030],\n",
            "        [0.1055],\n",
            "        [0.0901],\n",
            "        [0.1278],\n",
            "        [0.1314],\n",
            "        [0.1198],\n",
            "        [0.0973],\n",
            "        [0.0856],\n",
            "        [0.1630],\n",
            "        [0.1276],\n",
            "        [0.1226],\n",
            "        [0.0990],\n",
            "        [0.1595],\n",
            "        [0.0882],\n",
            "        [0.0935],\n",
            "        [0.0887],\n",
            "        [0.1023],\n",
            "        [0.1282],\n",
            "        [0.0980],\n",
            "        [0.1091],\n",
            "        [0.1117],\n",
            "        [0.1384],\n",
            "        [0.0996],\n",
            "        [0.1083],\n",
            "        [0.1112],\n",
            "        [0.1072],\n",
            "        [0.1267],\n",
            "        [0.0965],\n",
            "        [0.0918],\n",
            "        [0.0939],\n",
            "        [0.1053],\n",
            "        [0.1084],\n",
            "        [0.0979],\n",
            "        [0.0971],\n",
            "        [0.1056],\n",
            "        [0.1389],\n",
            "        [0.1384],\n",
            "        [0.1184],\n",
            "        [0.1118],\n",
            "        [0.1090],\n",
            "        [0.0947],\n",
            "        [0.1369],\n",
            "        [0.1129],\n",
            "        [0.1138],\n",
            "        [0.1220],\n",
            "        [0.1271],\n",
            "        [0.0971],\n",
            "        [0.1279],\n",
            "        [0.0861],\n",
            "        [0.1092],\n",
            "        [0.1208],\n",
            "        [0.1298],\n",
            "        [0.1124],\n",
            "        [0.0980],\n",
            "        [0.1003],\n",
            "        [0.1186],\n",
            "        [0.1071],\n",
            "        [0.1125],\n",
            "        [0.1180],\n",
            "        [0.1060],\n",
            "        [0.0985],\n",
            "        [0.1002],\n",
            "        [0.1451],\n",
            "        [0.0859],\n",
            "        [0.0981],\n",
            "        [0.0970]], grad_fn=<StdBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weight Initialization"
      ],
      "metadata": {
        "id": "RTBl1m9lPVjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constant/ Zero weight Initialization"
      ],
      "metadata": {
        "id": "01-51zTiPHp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),  # First hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(128, 64),  # Second hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(64, 1)  # Output layer\n",
        ")\n",
        "\n",
        "# Initialize weights and biases with zeros\n",
        "for layer in model.modules():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.constant_(layer.weight, 0)\n",
        "        nn.init.constant_(layer.bias, 0)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "    #ms\n",
        "\n",
        "    # Forward pass and loss computation\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Compute the gradients\n",
        "    optimizer.step()  # Update the parameters\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Inference mode, no gradient tracking\n",
        "        y_pred_test = model(X_test_tensor)\n",
        "        test_loss = loss_fn(y_pred_test, y_test_tensor)\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss.item():.3f}, Test Loss: {test_loss.item():.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VabnQkxLJOR8",
        "outputId": "5c600fdd-3f1a-4dbd-edc8-36c7431f536e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 18.756, Test Loss: 15.308\n",
            "Epoch: 1, Train Loss: 18.459, Test Loss: 15.065\n",
            "Epoch: 2, Train Loss: 18.173, Test Loss: 14.833\n",
            "Epoch: 3, Train Loss: 17.899, Test Loss: 14.611\n",
            "Epoch: 4, Train Loss: 17.636, Test Loss: 14.398\n",
            "Epoch: 5, Train Loss: 17.383, Test Loss: 14.195\n",
            "Epoch: 6, Train Loss: 17.140, Test Loss: 14.000\n",
            "Epoch: 7, Train Loss: 16.906, Test Loss: 13.814\n",
            "Epoch: 8, Train Loss: 16.682, Test Loss: 13.637\n",
            "Epoch: 9, Train Loss: 16.467, Test Loss: 13.466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uniform\n"
      ],
      "metadata": {
        "id": "dMDYNx3rPYve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),  # First hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(128, 64),  # Second hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(64, 1)  # Output layer\n",
        ")\n",
        "\n",
        "# Initialize weights and biases with Uniform Distribution\n",
        "for layer in model.modules():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.uniform_(layer.weight, -0.5, 0.5)\n",
        "        nn.init.uniform_(layer.bias, -0.5, 0.5)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "    # Forward pass and loss computation\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Compute the gradients\n",
        "    optimizer.step()  # Update the parameters\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Inference mode, no gradient tracking\n",
        "        y_pred_test = model(X_test_tensor)\n",
        "        test_loss = loss_fn(y_pred_test, y_test_tensor)\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss.item():.3f}, Test Loss: {test_loss.item():.3f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djL7D5G0NzjL",
        "outputId": "fbe54747-8694-44cb-c066-2067ab6d92d8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 23.377, Test Loss: 10.077\n",
            "Epoch: 1, Train Loss: 11.306, Test Loss: 7.638\n",
            "Epoch: 2, Train Loss: 8.730, Test Loss: 5.915\n",
            "Epoch: 3, Train Loss: 6.845, Test Loss: 4.610\n",
            "Epoch: 4, Train Loss: 5.406, Test Loss: 3.616\n",
            "Epoch: 5, Train Loss: 4.292, Test Loss: 2.859\n",
            "Epoch: 6, Train Loss: 3.428, Test Loss: 2.293\n",
            "Epoch: 7, Train Loss: 2.769, Test Loss: 1.875\n",
            "Epoch: 8, Train Loss: 2.273, Test Loss: 1.572\n",
            "Epoch: 9, Train Loss: 1.904, Test Loss: 1.355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal"
      ],
      "metadata": {
        "id": "cIutQwqyPbK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),  # First hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(128, 64),  # Second hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(64, 1)  # Output layer\n",
        ")\n",
        "\n",
        "# Initialize weights and biases with Normal Distribution\n",
        "for layer in model.modules():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "        nn.init.normal_(layer.bias, mean=0, std=0.01)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "    # Forward pass and loss computation\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Compute the gradients\n",
        "    optimizer.step()  # Update the parameters\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Inference mode, no gradient tracking\n",
        "        y_pred_test = model(X_test_tensor)\n",
        "        test_loss = loss_fn(y_pred_test, y_test_tensor)\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss.item():.3f}, Test Loss: {test_loss.item():.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h39P0ZshOIUB",
        "outputId": "3c71d3f7-42a9-46e2-941c-3e854830a009"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 18.826, Test Loss: 15.362\n",
            "Epoch: 1, Train Loss: 18.523, Test Loss: 15.115\n",
            "Epoch: 2, Train Loss: 18.232, Test Loss: 14.878\n",
            "Epoch: 3, Train Loss: 17.953, Test Loss: 14.653\n",
            "Epoch: 4, Train Loss: 17.685, Test Loss: 14.437\n",
            "Epoch: 5, Train Loss: 17.428, Test Loss: 14.230\n",
            "Epoch: 6, Train Loss: 17.182, Test Loss: 14.032\n",
            "Epoch: 7, Train Loss: 16.945, Test Loss: 13.843\n",
            "Epoch: 8, Train Loss: 16.717, Test Loss: 13.663\n",
            "Epoch: 9, Train Loss: 16.499, Test Loss: 13.490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Xavier/Glorot Initialization"
      ],
      "metadata": {
        "id": "ySWCiQkbPctH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),  # First hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(128, 64),  # Second hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(64, 1)  # Output layer\n",
        ")\n",
        "\n",
        "# Initialize weights and biases with Xavier/Glorot Initialization\n",
        "for layer in model.modules():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.xavier_uniform_(layer.weight)\n",
        "        #nn.init.xavier_uniform_(layer.bias)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "    # Forward pass and loss computation\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Compute the gradients\n",
        "    optimizer.step()  # Update the parameters\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Inference mode, no gradient tracking\n",
        "        y_pred_test = model(X_test_tensor)\n",
        "        test_loss = loss_fn(y_pred_test, y_test_tensor)\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss.item():.3f}, Test Loss: {test_loss.item():.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCI4hS4QPDQB",
        "outputId": "69124780-dc2b-41e5-c89a-a3af4165b7ba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 19.201, Test Loss: 13.669\n",
            "Epoch: 1, Train Loss: 16.529, Test Loss: 12.090\n",
            "Epoch: 2, Train Loss: 14.642, Test Loss: 10.893\n",
            "Epoch: 3, Train Loss: 13.162, Test Loss: 9.926\n",
            "Epoch: 4, Train Loss: 11.927, Test Loss: 9.155\n",
            "Epoch: 5, Train Loss: 10.906, Test Loss: 8.553\n",
            "Epoch: 6, Train Loss: 10.082, Test Loss: 8.073\n",
            "Epoch: 7, Train Loss: 9.415, Test Loss: 7.663\n",
            "Epoch: 8, Train Loss: 8.858, Test Loss: 7.287\n",
            "Epoch: 9, Train Loss: 8.368, Test Loss: 6.920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## He Initialization"
      ],
      "metadata": {
        "id": "AT-eAbA4QUez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),  # First hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(128, 64),  # Second hidden layer\n",
        "    nn.ReLU(),  # Activation function\n",
        "    nn.Linear(64, 1)  # Output layer\n",
        ")\n",
        "\n",
        "# Initialize weights and biases with He Initialization\n",
        "for layer in model.modules():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.kaiming_uniform_(layer.weight)\n",
        "        #nn.init.kaiming_uniform_(layer.bias)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "    # Forward pass and loss computation\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Compute the gradients\n",
        "    optimizer.step()  # Update the parameters\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Inference mode, no gradient tracking\n",
        "        y_pred_test = model(X_test_tensor)\n",
        "        test_loss = loss_fn(y_pred_test, y_test_tensor)\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss.item():.3f}, Test Loss: {test_loss.item():.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZTXck0IPpYA",
        "outputId": "56008012-a80a-4be8-8511-e47e78cb00e3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 21.994, Test Loss: 6.157\n",
            "Epoch: 1, Train Loss: 7.179, Test Loss: 3.608\n",
            "Epoch: 2, Train Loss: 4.024, Test Loss: 2.349\n",
            "Epoch: 3, Train Loss: 2.628, Test Loss: 1.591\n",
            "Epoch: 4, Train Loss: 1.834, Test Loss: 1.160\n",
            "Epoch: 5, Train Loss: 1.379, Test Loss: 0.923\n",
            "Epoch: 6, Train Loss: 1.119, Test Loss: 0.789\n",
            "Epoch: 7, Train Loss: 0.966, Test Loss: 0.712\n",
            "Epoch: 8, Train Loss: 0.870, Test Loss: 0.665\n",
            "Epoch: 9, Train Loss: 0.807, Test Loss: 0.633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Normalization"
      ],
      "metadata": {
        "id": "fefaXSsjQw7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model with batch normalization\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 128),\n",
        "    nn.BatchNorm1d(128),  # Batch normalization after first linear layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.BatchNorm1d(64),  # Batch normalization after second linear layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 1)\n",
        ")\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "    # Forward pass and loss computation\n",
        "    y_pred = model(X_train_tensor)\n",
        "    loss = loss_fn(y_pred, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()  # Compute the gradients\n",
        "    optimizer.step()  # Update the parameters\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Inference mode, no gradient tracking\n",
        "        y_pred_test = model(X_test_tensor)\n",
        "        test_loss = loss_fn(y_pred_test, y_test_tensor)\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch: {epoch}, Train Loss: {loss.item():.3f}, Test Loss: {test_loss.item():.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4UQOxwoQeFM",
        "outputId": "c58bce5d-3696-4521-b47e-0b16145d21e8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 20.253, Test Loss: 14.430\n",
            "Epoch: 1, Train Loss: 10.908, Test Loss: 13.049\n",
            "Epoch: 2, Train Loss: 6.110, Test Loss: 11.797\n",
            "Epoch: 3, Train Loss: 3.254, Test Loss: 10.739\n",
            "Epoch: 4, Train Loss: 1.725, Test Loss: 9.909\n",
            "Epoch: 5, Train Loss: 0.991, Test Loss: 9.283\n",
            "Epoch: 6, Train Loss: 0.654, Test Loss: 8.800\n",
            "Epoch: 7, Train Loss: 0.490, Test Loss: 8.400\n",
            "Epoch: 8, Train Loss: 0.400, Test Loss: 8.045\n",
            "Epoch: 9, Train Loss: 0.342, Test Loss: 7.709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ukGV1lvQyWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}